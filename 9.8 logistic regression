solve: classification problem!!!
estimation: estimate probabilities using logistic function
logistic function: map: all real number ---> [0,1]  y=1/(1+exp(-x))
after gaining probabilities, they must be transformed to binary values: 0, 1  it will use threshold classifier
output: discrete outcomes

1.data preprocessing
(1) import data
df. info
(2) handle missing value
(3) split data to training set and test set
(4) scaling features

2. logistic regression
LogisticRegression().fit(train_X, train_Y)

3. make prediction

4. evaluation the prediction

the story behind logistic regression!!!!!!!!

negative class(absence), positive class(present)

if linear regression:
1. hypothesis(theta) = theta' X
threshold  hthetax > 0.5, predict y =1 
but so big value appear, hypothesis function may change, threshold also changes
2. hypothesis(theta) can >1 or <0

logistic hypothesis representation:
want 0<= h(theta) <= 1
h(theta)(x) = g(theta'x),  g(z) =1/(1+exp(-z))  g: sigmoid function/logistic function
h(theta)(x) = 1/(1+exp(-theta'x))
parameter theta wait us to fit
h(theta)(x): estimated probability that y=1 on input x(probability that y=1, given x, parameterized by theta)

decision boundary
y =1: g(theta'x)) >=0.5  theta'x >0  theta'x = 0: decision boundary

non-linear decision boundary
theta0+theta1*x1+theta2*x2+theta3*x1^2+theta4*x2^2

cost function (why log? other convex function can't?)
how to choose parameter theta
linear: J(theta) = 1/m*sum(1/2*(htheta(xi)-yi)^2)
ligistic:  cost(htheta(x),y) = -log(htheta(x))  if y=1;  -log(1-htheta(x))   if y=0
cost = 0 if y = 1 and htheta(x) =1,  we will penalize algorithm by a very large cost

advanced optimization
want minimize cost function    gradient descent: learning rate alpha
conjugate gradient, BFGS, L-BFGS: no need to pick learning rate alpha,  faster than gradient descent
initial guess
function [jval, gradient] = costfunction(theta)
