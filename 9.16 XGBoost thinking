eXtreme Gradient Boosting  complicated, apt to many,  
many trees combination , gradient boosting machines C++ development 
fast, large-scale, efficient. won't let you down.
push the limit of computations resources for boosted tree algorithms

Gradient Boosting Machines
minimize target function. 
target function: 
(1) cost function: related to task. Regression: like mean squared error. Classification: 0-1 cost, logistic cost
(2) regular term: related to model complicity. L2 regular, L1 regular.  choose simplest model
boosting: combine weak learning machine
weak learning machine: better than random guess.  like decision tree/classification regression tree
decision tree: every leaf is correspond to a decision
calssification regression tree: every leaf has score, more flexible than decision tree

adaptive boosting, decision tree only has one depth. weight related to correct rate
GBM: regarding boosting as a numerical optimization problem, use gradient descend
stage-wise additive model: add weak learning machine, exsit weak learning machine don't move
we can use any derivatived cost function

XGBoost vs GBM:
1. regularized boosting ,  helpful to overfitting
2. Parallel processing， faster
3. higher flexibility, can define 
4. Pruning： added split gain negative effect, GBM stop spliting(greedy). XGBoost will split to max depth, then go prune
5. inner cross validation.  boosting iteration times--how many trees?
6. continuing study

advantage:
speed, model performance(on structured dataset)   why fast and perform well?? parallel processing, inner cross validation

set training parameters(hyper parameters）
features transformation
XGBoosting do well in Sparse feature. dtrain = xgb.DMatrix(filepath)
store and executing speed development
max_depth, eta(learning rate), silent, objective, num_round(how many trees)

how many parameters in XGBoost? how to determine the best

create learning machine(setting parameters) --->fit---> predict

learning curve:
model predict performance change with a parameter

early stop:
performance won't increase, end training process
bst.fit(early_stopping_rounds = 10)

cross validation:
one train_test_split will bring randomness
make training set to 3/5/10, 1,2train,3test; 1,3train, 2test; 2,3train;1test

GridSearchCV
calculate ev
